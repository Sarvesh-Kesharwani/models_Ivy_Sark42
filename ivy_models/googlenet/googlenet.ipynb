{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # %%script python -- arg1 arg2 arg3\n",
    "\n",
    "# import ivy\n",
    "# ivy.set_backend(\"torch\")\n",
    "# from googlenet import inceptionNet_v1\n",
    "# from ivy_models_tests import helpers\n",
    "# import torch\n",
    "\n",
    "# def test():\n",
    "#     model = inceptionNet_v1(pretrained=False)\n",
    "#     output = model(torch.randn(1, 3, 224, 224))\n",
    "#     print(output.size())\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global\n",
    "import builtins\n",
    "import ivy\n",
    "import ivy_models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the initial Convolutional Block\n",
    "class ConvBlock(ivy.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "    def _build(self, *args, **kwargs):\n",
    "        self.conv = ivy.Conv2D(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, with_bias=False)\n",
    "        self.bn = ivy.BatchNorm2D(self.out_channels)\n",
    "        self.activation = ivy.ReLU()\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Inception(ivy.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        num1x1,\n",
    "        num3x3_reduce,\n",
    "        num3x3,\n",
    "        num5x5_reduce,\n",
    "        num5x5,\n",
    "        pool_proj,\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.num1x1 = num1x1\n",
    "        self.num3x3_reduce = num3x3_reduce\n",
    "        self.num3x3 = num3x3\n",
    "        self.num5x5_reduce = num5x5_reduce\n",
    "        self.num5x5 = num5x5\n",
    "        self.pool_proj = pool_proj\n",
    "        super(Inception, self).__init__()\n",
    "    \n",
    "    def _build(self, *args, **kwargs):\n",
    "        self.block1 = ivy.Sequential(ConvBlock(self.in_channels, self.num1x1, kernel_size=[1, 1], stride=1, padding=0)\n",
    "                                     )\n",
    "        self.block2 = ivy.Sequential(ConvBlock(self.in_channels, self.num3x3_reduce, kernel_size=[1, 1], stride=1, padding=0),\n",
    "                                    ConvBlock(self.num3x3_reduce, self.num3x3, kernel_size=[3, 3], stride=1, padding=1)\n",
    "                                    )\n",
    "        self.block3 = ivy.Sequential(ConvBlock(self.in_channels, self.num5x5_reduce, kernel_size=[1, 1], stride=1, padding=0),\n",
    "                                    ConvBlock(self.num5x5_reduce, self.num5x5, kernel_size=[3, 3], stride=1, padding=1)\n",
    "                                    )\n",
    "        self.block4 = ivy.Sequential(ivy.MaxPool2D(3, 1, 1),\n",
    "                                    ConvBlock(self.in_channels, self.pool_proj, kernel_size=[1, 1], stride=1, padding=0)\n",
    "                                    )\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(x)\n",
    "        block3 = self.block3(x)\n",
    "        block4 = self.block4(x)\n",
    "\n",
    "        return ivy.concat([block1, block2, block3, block4], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Auxiliary(ivy.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        super(Auxiliary, self).__init__()\n",
    "\n",
    "    def _build(self, *args, **kwargs):\n",
    "        self.pool = ivy.AvgPool2D((5, 5), 3, 0)  # ivy.Shape(1, 4, 4, 512)\n",
    "        self.bn = ivy.BatchNorm2D(128)\n",
    "        self.conv = ivy.Conv2D(self.in_channels, 128, [1,1], 1, 0, with_bias=False)\n",
    "        self.activation = ivy.ReLU()\n",
    "        self.fc1 = ivy.Linear(2048, 1024)\n",
    "        self.dropout = ivy.Dropout(0.7)\n",
    "        self.fc2 = ivy.Linear(1024, self.num_classes)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out) # contains weights\n",
    "        out = self.activation(out)\n",
    "        out = ivy.flatten(out, start_dim=1)\n",
    "        out = self.fc1(out) # contains weights\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out) # contains weights\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogLeNet(ivy.Module):\n",
    "    def __init__(self, num_classes=1000, v: ivy.Container = None,):\n",
    "        if v is not None:\n",
    "            self.v = v\n",
    "        self.num_classes = num_classes\n",
    "        super(GoogLeNet, self).__init__(v=v)\n",
    "\n",
    "    def _build(self, *args, **kwargs):\n",
    "        self.conv1 = ConvBlock(3, 64, [7,7], 2, 3)\n",
    "        self.pool1 = ivy.MaxPool2D([3,3], 2, 1)\n",
    "        self.conv2 = ConvBlock(64, 64, [1,1], 1, 0,)\n",
    "        self.conv3 = ConvBlock(64, 192, [3,3], 1, 1)\n",
    "        self.pool3 = ivy.MaxPool2D(3, 2, 1)\n",
    "        self.inception3A = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3B = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.pool4 = ivy.MaxPool2D(3, 2, 1)\n",
    "\n",
    "        self.inception4A = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "\n",
    "        # ivy.flatten()\n",
    "        self.aux4A = Auxiliary(512, self.num_classes)\n",
    "\n",
    "        self.inception4B = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4C = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4D = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "\n",
    "        self.aux4D = Auxiliary(528, self.num_classes)\n",
    "\n",
    "        self.inception4E = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.pool5 = ivy.MaxPool2D(3, 2, 1)\n",
    "\n",
    "        self.inception5A = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5B = Inception(832, 384, 192, 384, 48, 128,128)\n",
    "        self.pool6 = ivy.AvgPool2D((7,7), 1, 0) # ((1, 1))\n",
    "\n",
    "        # ivy.flatten()\n",
    "        self.dropout = ivy.Dropout(0.4)\n",
    "        self.fc = ivy.Linear(1024, self.num_classes)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.pool3(out)\n",
    "        out = self.inception3A(out)\n",
    "        out = self.inception3B(out)\n",
    "        out = self.pool4(out)\n",
    "        out = self.inception4A(out)\n",
    "\n",
    "        aux1 = self.aux4A(out)\n",
    "\n",
    "        out = self.inception4B(out)\n",
    "        out = self.inception4C(out)\n",
    "        out = self.inception4D(out)\n",
    "\n",
    "        aux2 = self.aux4D(out)\n",
    "\n",
    "        out = self.inception4E(out)\n",
    "        out = self.pool5(out)\n",
    "        out = self.inception5A(out)\n",
    "        out = self.inception5B(out)\n",
    "        out = self.pool6(out)\n",
    "        out = ivy.flatten(out, start_dim=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, aux1, aux2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GoogLeNet()\n",
    "# model.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _inceptionNet_torch_weights_mapping(old_key, new_key):\n",
    "    W_KEY = [\"conv/weight\"]\n",
    "    new_mapping = new_key\n",
    "    if any([kc in old_key for kc in W_KEY]):\n",
    "        new_mapping = {\"key_chain\": new_key, \"pattern\": \"b c h w -> h w c b\"}\n",
    "    return new_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inceptionNet_v1(pretrained=True):\n",
    "    \"\"\"InceptionNet-V1 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return GoogLeNet()\n",
    "\n",
    "    reference_model = GoogLeNet()\n",
    "    url = \"https://download.pytorch.org/models/googlenet-1378be20.pth\"\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_inceptionNet_torch_weights_mapping,\n",
    "        )\n",
    "    display(\"calling model with weights!\")\n",
    "    return GoogLeNet(v=w_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_model = GoogLeNet()\n",
    "# url = \"https://download.pytorch.org/models/googlenet-1378be20.pth\"\n",
    "# w_clean = ivy_models.helpers.load_torch_weights(\n",
    "#     url,\n",
    "#     reference_model,\n",
    "#     raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "#     custom_mapping=_inceptionNet_torch_weights_mapping,\n",
    "#     )\n",
    "# display(w_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image \n",
    "\n",
    "def load_and_preprocess_img(\n",
    "    path, new_size, crop, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    img = Image.open(path)\n",
    "    compose = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(new_size),\n",
    "            transforms.CenterCrop(crop),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    img = compose(img)\n",
    "    img = img.unsqueeze(0).permute((0, 2, 3, 1))\n",
    "    return img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/cat-test/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Load and preprocess the image\u001b[39;00m\n\u001b[1;32m     36\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/kaggle/input/cat-test/cat.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\n\u001b[1;32m     38\u001b[0m input_tensor \u001b[39m=\u001b[39m preprocess(image)\n\u001b[1;32m     39\u001b[0m input_batch \u001b[39m=\u001b[39m input_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/fw/paddle/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cat-test/cat.jpg'"
     ]
    }
   ],
   "source": [
    "import ivy\n",
    "ivy.set_backend(\"torch\")\n",
    "\n",
    "# Load image\n",
    "this_dir = \"/models/images/cat.jpg\"\n",
    "img = ivy.asarray(load_and_preprocess_img(this_dir, 256, 224))\n",
    "display(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([282, 281, 285])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def inceptionNet_v1(pretrained=True):\n",
    "    \"\"\"InceptionNet-V1 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return models.googlenet(pretrained=False)\n",
    "\n",
    "    reference_model = models.googlenet()\n",
    "    url = \"https://download.pytorch.org/models/googlenet-1378be20.pth\"\n",
    "    w_clean = torch.hub.load_state_dict_from_url(url, progress=True)\n",
    "    # display(w_clean)\n",
    "    # Load weights into the non-pretrained model\n",
    "    model = models.googlenet()\n",
    "    model.load_state_dict(w_clean)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load GoogLeNet model\n",
    "model = inceptionNet_v1(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation for input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"/models/images/cat.jpg\"\n",
    "image = Image.open(image_path)\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "display(input_batch.shape)\n",
    "# Check if GPU is available, else use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    input_batch = input_batch.to(device)\n",
    "    output_gt = model(input_batch)\n",
    "\n",
    "    \n",
    "# display(model.state_dict)\n",
    "# display(output)\n",
    "# display(torch.max(output, 1))\n",
    "true_indices = torch.argsort(output_gt[0], descending=True)[:3]\n",
    "display(true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 65, 410, 356])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torch\n",
    "# from torchvision.models import googlenet\n",
    "\n",
    "# input_tensor = torch.reshape(img, (1, 3, 224, 224))\n",
    "# gt_model = googlenet()\n",
    "# gt_model.eval()\n",
    "\n",
    "# # Check if GPU is available, else use CPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = gt_model.to(device)\n",
    "# input_batch = input_tensor.to(device)\n",
    "\n",
    "# # Perform the forward pass to get predictions\n",
    "# with torch.no_grad():\n",
    "#     input_batch = input_batch.to(device)\n",
    "#     output_gt = model(input_batch)\n",
    "\n",
    "# output_gt.shape\n",
    "# true_indices = torch.argsort(output_gt[0], descending=True)[:3]\n",
    "# display(true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calling model with weights!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = inceptionNet_v1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ivy.Shape(1, 1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_tensor = ivy.reshape(input_tensor, (1, 224, 224, 3))\n",
    "output, _, _ = model(img)\n",
    "\n",
    "display(output_gt.shape)\n",
    "display(output.shape)\n",
    "# display(model.v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output.shape == tuple([1, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivy.array([282, 281, 285])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([282, 281, 285])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# indices of logits in decending order from both outputs\n",
    "calc_indices = ivy.argsort(output[0], descending=True)[:3]\n",
    "# true_indices = ivy.argsort(output_gt[0], descending=True)[:3]\n",
    "display(calc_indices)\n",
    "display(true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(true_indices, calc_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivy.array([5.95262003, 5.32760286, 4.96759605])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.73805046081543"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calc_logits = ivy.take_along_axis(output[0], calc_indices, 0)\n",
    "true_logits = output_gt[0, true_indices[0]].tolist()\n",
    "display(calc_logits)\n",
    "display(true_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(true_logits, calc_logits, rtol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
