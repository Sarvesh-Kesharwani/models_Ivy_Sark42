{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Type, Union\n",
    "import builtins\n",
    "\n",
    "import ivy\n",
    "import ivy_models\n",
    "from layers import conv1x1, BasicBlock, Bottleneck\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global\n",
    "from typing import List, Optional, Type, Union\n",
    "import builtins\n",
    "\n",
    "import ivy\n",
    "import ivy_models\n",
    "# from layers import conv1x1, BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "class ResNet(ivy.Module):\n",
    "    \"\"\"\n",
    "    Residual Neural Network (ResNet) architecture.\n",
    "\n",
    "    Args::\n",
    "        block (Type[Union[BasicBlock, Bottleneck]]):\n",
    "            The block type used in the ResNet architecture.\n",
    "        layers: List of integers specifying the number of blocks in each layer.\n",
    "        num_classes (int): Number of output classes. Defaults to 1000.\n",
    "        base_width (int): The base width of the ResNet. Defaults to 64.\n",
    "        replace_stride_with_dilation (Optional[List[bool]]):\n",
    "            List indicating whether to replace stride with dilation.\n",
    "        v (ivy.Container): Unused parameter. Can be ignored.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        base_width: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        v: ivy.Container = None,\n",
    "    ) -> None:\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.block = block\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes\n",
    "        # if replace_stride_with_dilation is None:\n",
    "        if replace_stride_with_dilation is None:\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        self.replace_stride_with_dilation = replace_stride_with_dilation\n",
    "\n",
    "        self.base_width = base_width\n",
    "        super(ResNet, self).__init__(v=v)\n",
    "\n",
    "    def _build(self, *args, **kwargs):\n",
    "        self.conv1 = ivy.Conv2D(3, self.inplanes, [7, 7], 2, 3, with_bias=False)\n",
    "        self.bn1 = ivy.BatchNorm2D(self.inplanes)\n",
    "        self.relu = ivy.ReLU()\n",
    "        self.maxpool = ivy.MaxPool2D(3, 2, 1)\n",
    "        self.layer1 = self._make_layer(self.block, 64, self.layers[0])\n",
    "        self.layer2 = self._make_layer(\n",
    "            self.block,\n",
    "            128,\n",
    "            self.layers[1],\n",
    "            stride=2,\n",
    "            dilate=self.replace_stride_with_dilation[0],\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            self.block,\n",
    "            256,\n",
    "            self.layers[2],\n",
    "            stride=2,\n",
    "            dilate=self.replace_stride_with_dilation[1],\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            self.block,\n",
    "            512,\n",
    "            self.layers[3],\n",
    "            stride=2,\n",
    "            dilate=self.replace_stride_with_dilation[2],\n",
    "        )\n",
    "        self.avgpool = ivy.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = ivy.Linear(512 * self.block.expansion, self.num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> ivy.Sequential:\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = ivy.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                ivy.BatchNorm2D(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes,\n",
    "                planes,\n",
    "                stride,\n",
    "                downsample,\n",
    "                self.base_width,\n",
    "                previous_dilation,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return ivy.Sequential(*layers)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        dtype = x.dtype\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = ivy.asarray(x, dtype=dtype)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = ivy.permute_dims(x, (0, 3, 1, 2))\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global\n",
    "import ivy\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _resnet_torch_weights_mapping(old_key, new_key):\n",
    "    # display(\"\\n##################################################################################################\\n\")\n",
    "    # display(old_key)\n",
    "    # display(\"\\n##################################################################################################\\n\")\n",
    "    # display(new_key)\n",
    "    # display(\"\\n##################################################################################################\\n\")\n",
    "\n",
    "    W_KEY = [\"conv1/weight\", \"conv2/weight\", \"conv3/weight\", \"downsample/0/weight\"]\n",
    "    new_mapping = new_key\n",
    "    if any([kc in old_key for kc in W_KEY]):\n",
    "        new_mapping = {\"key_chain\": new_key, \"pattern\": \"b c h w -> h w c b\"}\n",
    "    return new_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _map_weights(raw, ref, custom_mapping=None):\n",
    "    mapping = {}\n",
    "    for old_key, new_key in zip(\n",
    "        raw.cont_sort_by_key().cont_to_iterator_keys(),\n",
    "        ref.cont_sort_by_key().cont_to_iterator_keys(),\n",
    "    ):\n",
    "        new_mapping = new_key\n",
    "        if custom_mapping is not None:\n",
    "            new_mapping = custom_mapping(old_key, new_key)\n",
    "            if new_mapping is None:\n",
    "                continue\n",
    "        mapping[old_key] = new_mapping\n",
    "    return mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prune_keys(raw, ref, raw_keys_to_prune=[], ref_keys_to_prune=[]):\n",
    "    if raw_keys_to_prune != []:\n",
    "        raw = raw.cont_prune_keys(raw_keys_to_prune) \n",
    "        # check if the raw container contains these raw_keys_to_prune keychains,\n",
    "        # if yes, removes theme from raw container and returns updated container,\n",
    "        # if no, return same container as it is.     \n",
    "    if ref_keys_to_prune != []:\n",
    "        ref = ref.cont_prune_keys(ref_keys_to_prune)\n",
    "    return raw, ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_torch_weights(\n",
    "    url,\n",
    "    ref_model,\n",
    "    raw_keys_to_prune=[],\n",
    "    ref_keys_to_prune=[],\n",
    "    custom_mapping=None,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "):\n",
    "    ivy.set_backend(\"torch\")\n",
    "    weights = torch.hub.load_state_dict_from_url(url, map_location=map_location)\n",
    "\n",
    "    weights_raw = ivy.to_numpy(ivy.Container(weights))\n",
    "    weights_raw, weights_ref = _prune_keys(\n",
    "        weights_raw, ref_model.v, raw_keys_to_prune, ref_keys_to_prune\n",
    "    )\n",
    "    # raw == downloaded model\n",
    "    # ref == my custom arch model\n",
    "    \n",
    "    mapping = _map_weights(weights_raw, weights_ref, custom_mapping=custom_mapping)\n",
    "\n",
    "    ivy.previous_backend()\n",
    "    w_clean = weights_raw.cont_restructure(mapping, keep_orig=False)\n",
    "    return ivy.asarray(w_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resnet_18(pretrained=True):\n",
    "    \"\"\"ResNet-18 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "    reference_model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    url = \"https://download.pytorch.org/models/resnet18-f37072fd.pth\"\n",
    "\n",
    "    # downlaods weights, cleans it based on passed parameters, and returns the clean\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_resnet_torch_weights_mapping,\n",
    "    )\n",
    "\n",
    "    display(f\"cleaned weights are: {w_clean}\")\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], v=w_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given code snippet, the line raw_keys_to_prune=[\"num_batches_tracked\"] is a parameter being passed to the load_torch_weights function. This parameter specifies a list of keys that should be pruned from the loaded Torch weights before assigning them to the reference_model.\n",
    "\n",
    "The purpose of specifying raw_keys_to_prune is to remove certain keys from the loaded weights, which allows customization or modification of the pretrained model. In this case, the key \"num_batches_tracked\" is being specified to be pruned from the loaded weights.\n",
    "\n",
    "By pruning the \"num_batches_tracked\" key, it is possible to initialize the reference_model with the pretrained weights while excluding the accumulated batch count information. This can be useful when the model is intended to be fine-tuned or used for inference only, rather than continuing the training from where it left off.\n",
    "\n",
    "Overall, the purpose of this part of the code is to load pretrained weights for the ResNet-18 model, with the option to prune certain keys from the loaded weights, specifically the \"num_batches_tracked\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet_34(pretrained=True):\n",
    "    \"\"\"ResNet-34 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "    reference_model = ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "    url = \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\"\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_resnet_torch_weights_mapping,\n",
    "    )\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], v=w_clean)\n",
    "\n",
    "\n",
    "def resnet_50(pretrained=True):\n",
    "    \"\"\"ResNet-50 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    reference_model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    url = \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\"\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_resnet_torch_weights_mapping,\n",
    "    )\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], v=w_clean)\n",
    "\n",
    "\n",
    "def resnet_101(pretrained=True):\n",
    "    \"\"\"ResNet-101 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    reference_model = ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "    url = \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\"\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_resnet_torch_weights_mapping,\n",
    "    )\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], v=w_clean)\n",
    "\n",
    "\n",
    "def resnet_152(pretrained=True):\n",
    "    \"\"\"ResNet-152 model\"\"\"\n",
    "    if not pretrained:\n",
    "        return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "    reference_model = ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "    url = \"https://download.pytorch.org/models/resnet152-f82ba261.pth\"\n",
    "    w_clean = ivy_models.helpers.load_torch_weights(\n",
    "        url,\n",
    "        reference_model,\n",
    "        raw_keys_to_prune=[\"num_batches_tracked\"],\n",
    "        custom_mapping=_resnet_torch_weights_mapping,\n",
    "    )\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], v=w_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cleaned weights are: \\x1b[34m{\\x1b[0m\\n    \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n        \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n        \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n        \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64])\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[7, 7, 3, 64])\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mfc\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[1000]),\\n        \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[1000, 512])\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mlayer1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n            \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 64, 64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 64, 64])\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m,\\n            \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 64, 64])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 64, 64])\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m\\n        \\x1b[34m}\\x1b[0m\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mlayer2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n            \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 64, 128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 128, 128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mdownsample\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                        \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[1, 1, 64, 128])\\n                        \\x1b[34m}\\x1b[0m,\\n                        \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                            \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                            \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128])\\n                        \\x1b[34m}\\x1b[0m\\n                    \\x1b[34m}\\x1b[0m\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m,\\n            \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 128, 128])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 128, 128])\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m\\n        \\x1b[34m}\\x1b[0m\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mlayer3\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n            \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 128, 256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 256, 256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mdownsample\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                        \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[1, 1, 128, 256])\\n                        \\x1b[34m}\\x1b[0m,\\n                        \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                            \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                            \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256])\\n                        \\x1b[34m}\\x1b[0m\\n                    \\x1b[34m}\\x1b[0m\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m,\\n            \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 256, 256])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 256, 256])\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m\\n        \\x1b[34m}\\x1b[0m\\n    \\x1b[34m}\\x1b[0m,\\n    \\x1b[32mlayer4\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n        \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n            \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 256, 512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 512, 512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mdownsample\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32msubmodules\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                        \\x1b[32mv0\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[1, 1, 256, 512])\\n                        \\x1b[34m}\\x1b[0m,\\n                        \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                            \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                            \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                            \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                            \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512])\\n                        \\x1b[34m}\\x1b[0m\\n                    \\x1b[34m}\\x1b[0m\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m,\\n            \\x1b[32mv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                \\x1b[32mbn1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mbn2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mb\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_mean\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mrunning_var\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512]),\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv1\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 512, 512])\\n                \\x1b[34m}\\x1b[0m,\\n                \\x1b[32mconv2\\x1b[0m\\x1b[35m:\\x1b[0m \\x1b[34m{\\x1b[0m\\n                    \\x1b[32mw\\x1b[0m\\x1b[35m:\\x1b[0m (<\\x1b[34mclass\\x1b[0m ivy.data_classes.array.array.Array> \\x1b[35mshape=\\x1b[0m[3, 3, 512, 512])\\n                \\x1b[34m}\\x1b[0m\\n            \\x1b[34m}\\x1b[0m\\n        \\x1b[34m}\\x1b[0m\\n    \\x1b[34m}\\x1b[0m\\n\\x1b[34m}\\x1b[0m'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n",
      "WARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "model = resnet_18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
